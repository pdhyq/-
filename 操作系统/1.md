简答6题： 线程进程 程序的结构 临界资源的访问方法 批处理和分布式系统的原理区别 标注输入重定向和管道  

大题10分：程序的运行过程中，操作系统做了什么事    

​                    页面调度的算法

​					死锁，银行家算法

​					代码的分析，哪里为什么这么干，哪里进栈

​					文件系统

​					线程的调度，调度算法（给一个坐标轴，画出线程的运行情况）线程状态的转换（三状态，五状态）

​					同步，信号量，互斥





虚拟化让许多程序运行（从而共享 CPU），让许多程序可以同时访问自己的指令和数据（从而共享内存），让许多程序访问设备（从而共享磁盘等），所以操作系统有时被称为资源操理器（resource manager）。每个 CPU、内存和磁盘都是系统的资源（resource），因此操作系统扮演的主要角色就是操理（manage）这些资源，以做到高效或公平，或者实际上考虑其他许多可能的目标。

为了理解构成进程的是什么，我们必须理解它的机器状态（machine state）：程序在运行时可以读取或更新的内容。

fork（）函数：

 fork是复制进程的函数，程序一开始就会产生一个进程，当这个进程(代码)执行到fork()时，fork就会复制一份原来的进程即就是创建一个新进程,我们称子进程，而原来的进程我们称为父进程，此时父子进程是共存的，他们一起向下执行代码。

**注意的一点：**就是调用[fork函数](https://so.csdn.net/so/search?q=fork函数&spm=1001.2101.3001.7020)之后，一定是**两个进程同时执行fork函数之后的代码，而之前的代码以及由父进程执行完毕。**

UNIX 管道：

UNIX 管道也是用类似的方式实现的，但用的是 pipe()系统调用。谁这种情况下，一个 进程的输出被链接到了一个内核管谁（pipe）上（队列），另一个进程的输入也被连接到了 同一个管谁上。因此，前一个进程的输出无缝地作为后一个进程的输入，许多命令可以用 这种方式串联谁一起，共同完成某项任务。比如通过将 grep、wc 命令用管谁连接可以完成 从一个文件中查找某个词，并统计其出现次数的功能：grep -o foo file | wc -l。

|                       | 要做什么                                                     | 调度发生在。。。       | 发生频率 | 对进程状态的影响                  |
| --------------------- | ------------------------------------------------------------ | ---------------------- | -------- | --------------------------------- |
| 高级调度 （作业调度） | 按照某种规则，从后备队列 中选择合适的作业将其调入 内存，并为其创建进程 | 外存➡内存 （面向作业） | 低       | 无➡创建态➡就绪态                  |
| 中级调度 （内存调度） | 按照某种规则，从挂起队列 中选择合适的进程将其数据 调回内存   | 外存➡内存 （面向进程） | 中       | 挂起态➡就绪态 （阻塞挂起➡阻塞态） |
| 低级调度 （进程调度） | 按照某种规则，从就绪队列 中选择一个进程为其分配处 理机       | 内存➡CPU               | 高       | 就绪态➡运行态                     |

内核通过在启动时设置**陷阱表（trap table）**来实现。当机器启动时，它在特权（内核）模 式下执行，因此可以根据需要自由配置机器硬件。==操作系统做的第一件事，就是告诉硬件在 发生某些异常事件时要运行哪些代码。==例如，当发生硬盘中断，发生键盘中断或程序进行系 统调用时，应该运行哪些代码？==操作系统通常通过某种特殊的指令，通知硬件这些陷阱处理 程序的位置。**一旦硬件被通知，它就会记住这些处理程序的位置，直到下一次重新启动机器，** 并且硬件知道在发生系统调用和其他异常事件时要做什么（即跳转到哪段代码）。==(意思应该是这些硬件每次开机之后记住这些位置，然后直到关机，下次开机再重新记忆)

 <img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230420001252128.png" alt="image-20230420001252128" style="zoom: 80%;" />



LDE 协议：

第一个阶段（在系统引导时），内核初始化陷阱表，并且 CPU 记住它 的位置以供随后使用。内核通过特权指令来执行此操作（所有特权指令均以粗体突出显示）。

第 二个阶段（运行进程时），在使用从陷阱返回指令开始执行进程之前，内核设置了一些内容（例 如，在进程列表中分配一个节点，分配内存）。这会将 CPU 切换到用户模式并开始运行该进程。 当进程希望发出系统调用时，它会重新陷入操作系统，然后再次通过从陷阱返回，将控制权还给 进程。该进程然后完成它的工作，并从 main()返回。这通常会返回到一些存根代码，它将正确退 出该程序（例如，通过调用 exit()系统调用，这将陷入 OS 中）。此时，OS 清理干净，任务完成了。



上下文切换：上下文切换在概念上很简单：操作系统要做的就是为当前正在执行的进程保存一些寄存器 的值（例如，到它的内核栈），并为即将执行的进程恢复一些寄存器的值（从它的内核栈）。



### 调度（动词）

* 响应时间定义为从任务到达系统到首次运行的时间。更正式的定义是：
  *  T 响应时间= T 首次运行−T 到达时间 

* 周转时间（turnaround time）。任务的周转 时间定义为任务完成时间减去任务到达系统的时间。更正式的周转时间定义 T 周转时间是： 
  * T 周转时间= T 完成时间−T 到达时间



护航效应：

FIFO引起的，这个问题通常被称为护航效应（convoy effect）[B+79]，一些耗时较少的潜在资源消费 者被排在重量级的资源消费者之后。这个调度方案可能让你想起在杂货店只有一个排队队 伍的时候，如果看到前面的人装满 3 辆购物车食品并且掏出了支票本，你感觉如何？这会 等很长时间①。



<font size=32>$\Downarrow$</font>

为了解决护航效应，引出了最短任务优先（SJF）



还是因为护航问题p65，引出了最短完成时间优先（STCF）每当新工作进入系统时，它就会确定剩余工作和新工作中， 谁的**剩余时间**最少，然后调度该工作。

因为响应时间问题，引出了轮转（Round-Robin， RR）调度

摊销技术

我们介绍了调度的基本思想，并开发了两类方法。第一类是运行最短的工作，从而优 化周转时间。第二类是交替运行所有工作，从而优化响应时间。但很难做到“鱼与熊掌兼 得”，这是系统中常见的、固有的折中



多级反馈队列（Multi-level Feedback Queue， MLFQ）

* 规则 1：如果 A 的优先级 > B 的优先级，运行 A（不运行 B）。 
* 规则 2：如果 A 的优先级 = B 的优先级，轮转运行 A 和 B。

* 规则 3：工作进入系统时，放在最高优先级（最上层队列）。 

* 规则 4a：工作用完整个时间片后，降低其优先级（移入下一个队列）。 

* 规则 4b：如果工作在其时间片以内主动释放 CPU， 则优先级不变。

如果不知道工作是短工 作还是长工作，那么就在开始的时候假设其是短工作，并赋予最高优先级。如果确实是短 工作，则很快会执行完毕，否则将被慢慢移入低优先级队列，而这时该工作也被认为是长 工作了。通过这种方式，MLFQ 近似于 SJF。

缺点：

1. 首先，会有饥饿（starvation）问题。如果系统有“太多”交互型工作，就会不断占用 CPU，导致长工作永远无法得到 CPU（它们饿死了）。即使在这种情况下，我们希望这些长 工作也能有所进展。 

2. 其次，聪明的用户会重写程序，愚弄调度程序（game the scheduler）。愚弄调度程序指 的是用一些卑鄙的手段欺骗调度程序，让它给你远超公平的资源。上述算法对如下的攻击 束手无策：进程在时间片用完之前，调用一个 I/O 操作（比如访问一个无关的文件），从而 主动释放 CPU。如此便可以保持在高优先级，占用更多的 CPU 时间。做得好时（比如，每 运行 99%的时间片时间就主动放弃一次 CPU），工作可以几乎独占 CPU。

解决方法：

一、提升优先级

一个简单的思路是周期性地提升（boost）所有工作的优先级。最简单的：将所有工作扔到最高优先级队列，经过一段时间 S，就将系统中所有工作重新加入最高优先级队列。（ S 为“巫毒常量（voo-doo constant）”，因为似乎需要一些黑魔法才能正确设置。==**如果 S 设置得太高，长工作会饥饿；如果设置得太低，交互型工作又得不到合适的 CPU 时间比例。**==）

二、更好的计时方式

调度程序应该记录一个进程在某一层中消耗的总时间，而不是在调度时重新计时。只要进 程用完了自己的配额，就将它降到低一优先级的队列中去。不论它是一次用完的，还是拆 成很多次用完。

 重新拟定规则 4：一旦工作用完了其在某一层中的时间配额（无论中间主动放弃了多少次 CPU），就降低其优先级（移入低一级队列）。



最终的MLFQ 规则：

* 规则 1：如果 A 的优先级 > B 的优先级，运行 A（不运行 B）。 
* 规则 2：如果 A 的优先级 = B 的优先级，轮转运行 A 和 B。 
* 规则 3：工作进入系统时，放在最高优先级（最上层队列）。 
* 规则 4：一旦工作用完了其在某一层中的时间配额（无论中间主动放弃了多少次 CPU），就降低其优先级（移入低一级队列）。 
* 规则 5：经过一段时间 S，就将系统中所有工作重新加入最高优先级队列。

### 比例份额（proportional-share）调度

比例份额（proportional-share）调度 程序，有时也称为公平份额（fair-share）调度程序。比例份额算法基于一个简单的想法：调 度程序的最终目标，是**确保每个工作获得一定比例的 CPU 时间，而不是优化周转时间和响 应时间。**

基本概念：彩票数表示份额

彩票机制:彩票调度还提供了一些机制，以不同且有效的方式来调度彩票。

* 一种方式是利用彩票 货币（ticket currency）的概念。这种方式允许拥有一组彩票的用户以他们喜欢的某种货币， 将彩票分给自己的不同工作。之后操作系统再自动将这种货币兑换为正确的全局彩票。
* 另一个有用的机制是彩票转让（ticket transfer）。通过转让，一个进程可以临时将自己 的彩票交给另一个进程。这种机制在客户端/服务端交互的场景中尤其有用，在这种场景中， 客户端进程向服务端发送消息，请求其按自己的需求执行工作，为了加速服务端的执行， 客户端可以将自己的彩票转让给服务端，从而尽可能加速服务端执行自己请求的速度。服 务端执行结束后会将这部分彩票归还给客户端。
* 最后，彩票通胀（ticket inflation）有时也很有用。利用通胀，一个进程可以临时提升或 降低自己拥有的彩票数量。当然在竞争环境中，进程之间互相不信任，这种机制就没什么 意义。一个贪婪的进程可能给自己非常多的彩票，从而接管机器。但是，通胀可以用于进 程之间相互信任的环境。在这种情况下，如果一个进程知道它需要更多 CPU 时间，就可以 增加自己的彩票，从而将自己的需求告知操作系统，这一切不需要与任何其他进程通信。

步长调度：

从上面的内容可以看出，虽然随机方式 可以使得调度程序的实现简单（且大致正确），但偶尔并不能产生正确的比例，尤其在工作 运行时间很短的情况下。由于这个原因，Waldspurger 提出了步长调度（stride scheduling）， 一个确定性的公平分配算法[W95]。

​		系统中的每个工作都有自己的**步长**，这个值与票数值成反比。在 上面的例子中，A、B、C 这 3 个工作的票数分别是 100、50 和 250，我们通过用一个大数 分别除以他们的票数来获得每个进程的步长。比如用 10000 除以这些票数值，得到了 3 个 进程的步长分别为 100、200 和 40。我们称这个值为每个进程的步长（stride）。每次进程运 行后，**我们会让它的计数器 [称为行程（pass）值] 增加它的步长，记录它的总体进展。 **（这里说的很混乱，看下边的高亮，是在把计数器的值增加一个步长）

​		之后，调度程序使用进程的步长及行程值来确定调度哪个进程。基本思路很简单：==当需要进行调度时，选择目前拥有最小行程值的进程，并且在运行之后将该进程的行程值增加一个步长。==

你可能想知道，既然有了可以精确控制的步长调度算法，为什么还要彩票调度算法 呢？好吧，彩票调度有一个步长调度没有的优势——不需要全局状态。假如一个新的进 程在上面的步长调度执行过程中加入系统，应该怎么设置它的行程值呢？设置成 0 吗？ 这样的话，它就独占 CPU 了。而彩票调度算法不需要对每个进程记录全局状态，只需要 用新进程的票数更新全局的总票数就可以了。因此彩票调度算法能够更合理地处理新加入的进程。（意思应该是，彩票调度和步长调度不是同一个调度，而是两种不同的调度，所以有的时候用步长调度不合适，就像这种有新进程介入的情况，这时候就可以用彩票调度，通过随机算法可以防止独占）

### 多处理器调度（高级）

为了理解多处理器调度带来的新问题，必须先知道它与单 CPU 之间的基本区别。区别 的核心在于对硬件缓存（cache）的使用（见图 10.1），以及多处理器之间共享数据的方式。

<img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230424100527403.png" alt="image-20230424100527403" style="zoom: 67%;" />

**缓存的时间局部性，空间局部性：**

缓存是基于局部性（locality）的概念，局部性有两种，即时间局部性和空间局部性。==时间局部性是指当一个数据被访问后，它很有可能会在不久的将来被再次访问，比如循 环代码中的数据或指令本身。而空间局部性指的是，当程序访问地址为 x 的数据时，很 有可能会紧接着访问 x 周围的数据，比如遍历数组或指令的顺序执行。==由于这两种局部 性存在于大多数的程序中，**硬件系统可以很好地预测哪些数据可以放入缓存**，从而运行 得很好。

系统有多个处理器，并共享同一个内存会引发什么问题

* 缓存一致性（cache coherence）问题，因为写回策略，CPU1修改了一个内存的值但是目前只修改了在cache中的数值，还没有写回到内存，CPU2在读取相同的地址的值时，使用的还是以前的未修改的值

  > 硬件提供了这个问题的基本解决方案：通过监控内存访问，硬件可以保证获得正确的 数据，并保证共享内存的唯一性。在基于总线的系统中，一种方式是使用**总线窥探**（bus  10.2 别忘了同步 75  snooping）[G83]。**每个缓存都通过监听链接所有缓存和内存的总线，来发现内存访问。如 果 CPU 发现对它放在缓存中的数据的更新，会作废（invalidate）本地副本（从缓存中移除）， 或更新（update）它（修改为新值）。**



缓存亲和度:

一个进程在某个 CPU 上运行时，会在该 CPU 的缓存中维护许多状态。**下次 该进程在相同 CPU 上运行时，由于缓存中的数据而执行得更快**。相反，**在不同的 CPU 上执 行，会由于需要重新加载数据而很慢**（好在硬件保证的缓存一致性可以保证正确执行）。因 此多处理器调度应该考虑到这种缓存亲和性，并尽可能将进程保持在同一个 CPU 上。（家都搬好了，就要回家住，要不然还要重新搬家太麻烦）

#### 多处理器系统的调度程序：

* 单队列调度SQMS

  > 如何设计一个多处理器系统的调度程序。最基本的 方式是简单地复用单处理器调度的基本架构，将所有需要调度的工作放入一个单独的队列 中，我们称之为单队列多处理器调度（Single Queue Multiprocessor Scheduling，SQMS）



### 抽象：地址空间

一种实现时分共享的方法，是让一个进程单独占用全部内存运行一小段时间（见图 13.1），然后停止它，并将它所有的状态信息保存在磁盘上（包含所有的物理内存），加载其 他进程的状态信息，再运行一段时间，这就实现了某种比较粗糙的机器共享

这种方法有一个问题：太慢了，特别是当内存增长的时候。虽然保存和恢复寄存器级的状态信息（程序计数器、通用寄存器等）相对较快，但将全部的内存信息保存到磁盘就太慢了。因此，在进程 切换的时候，我们仍然将进程信息放在内存中，这样操作系统 可以更有效率地实现时分共享（见图 13.2）。

随着时分共享变得更流行，人们对操作系统又有了新的要 求。特别是多个程序同时驻留在内存中，使保护成为重要问题。

为了解决上述问题，**操作系统需要提供一个易用 （easy to use）的物理内存抽象。这个抽象叫作<font color=red>地址空间</font>（address space），是运行的程序看到 的系统中的内存。**

==一个进程的地址空间包含运行的程序的所有内存状态。==

<img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230424165405160.png" alt="image-20230424165405160" style="zoom:50%;" />

代码是静态的（代码就那么长，不会变了，因此很容易放在内存中），所以可以将它放在地址空间的顶部

接下来，在程序运行时，地址空间有两个区 域可能增长（或者收缩）。它们就是堆（在顶部） 和栈（在底部）。把它们放在那里，是因为它们都希望能够增长。通过将它们放在地址空间 的两端，我们可以允许这样的增长：它们只需要在相反的方向增长。因此堆在代码（1KB） 之下开始并向下增长（当用户通过 malloc()请求更多内存时），栈从 16KB 开始并向上增长（当用户进行程序调用时）

**其实，当我们描述地址空间时，所描述的是操作系统提供给运行程序的抽象（abstract）。 程序不在物理地址 0～16KB 的内存中，而是加载在任意的物理地址。**

就引出了关键问题：**如何虚拟化内存** 

操作系统如何在单一的物理内存上为多个运行的进程（所有进程共享内存）构建一个==私有的、可能 很大的地址空间的抽象==？当操作系统这样做时，我们说操作系统在虚拟化内存（virtualizing memory），因为==**运行的程序**认为它被加载到特定地址（例如 0）的内存中，并且具有非常大的地址空间（例如 32 位或 64 位）==。**现实很不一样**（分清楚虚拟和现实，运行的程序只能看到虚拟，看不到现实，虚拟中他认为自己被加载到地址0，实际可以是物理地址302KB）

例如，当图 13.2 中的进程 A 尝试在地址 0（我们将称其为虚拟地址，virtual address） 执行加载操作时，然而操作系统在硬件的支持下，出于某种原因，==必须确保不是加载到物理地址 0，而是物理地址 320KB==（这是 A 载入内存的地址）。**这是内存虚拟化的关键，这是世界上每一个现代计算机系统的基础。**

> 隔离原则： 
>
> **隔离是建立可靠系统的关键原则**。如果两个实体相互隔离，这意味着一个实体的失败不会影响另一 个实体。操作系统力求让进程彼此隔离，从而防止相互造成伤害。通过内存隔离，操作系统进一步确保 运行程序不会影响底层操作系统的操作。一些现代操作系统通过将某些部分与操作系统的其他部分分 离，实现进一步的隔离。这样的微内核（microkernel）[BH70，R+89，S+03] 可以比整体内核提供更大 的可靠性。

* 虚拟内存（VM）系统的一个主要目标是透明$（transparency）^①$。操作系统实现虚拟内 存的方式，应该让运行的程序**看不见**。因此，程序不应该感知到内存被虚拟化的事实，相 反，程序的行为就好像它拥有自己的私有物理内存。在幕后，操作系统（和硬件）完成了 所有的工作，让不同的工作复用内存，从而实现这个假象。

* 虚拟内存的另一个目标是效率（efficiency）。操作系统应该追求虚拟化尽可能高效 （efficient），包括时间上（即不会使程序运行得更慢）和空间上（即不需要太多额外的内存 来支持虚拟化）。在实现高效率虚拟化时，操作系统将不得不依靠硬件支持，包括 TLB 这样 的硬件功能（我们将在适当的时候学习）。 

* 最后，虚拟内存第三个目标是保护（protection）。操作系统应确保进程受到保护（protect）， 不会受其他进程影响，操作系统本身也不会受进程影响。**当一个进程执行加载、存储或指令提取时，它不应该以任何方式访问或影响任何其他进程或操作系统本身的内存内容（即在它的地址空间之外的任何内容）**。因此，保护让我们能够在进程之间提供        **<u>隔离</u>**（isolation） 的特性，==每个进程都应该在自己的独立环境中运行，避免其他出错或恶意进程的影响。==

> 补充你看到的所有地址都不是真的，因为运行的程序只能看到自己虚拟化的内存，真实的物理地址是看不到的。**作为用户级程序的程序员，可以看到的任何地址都是虚拟地址。只有操作系统，通过精妙的虚拟化内存技术，知道这些指令和数据所在的物理内存的位置。所以永远不要忘记：==如果你在一个程序中打印出一个地址，那就是一个虚拟的地址。虚拟地址只是提供地址如何在内存中分布 的假象，只有操作系统（和硬件）才知道物理地址。==**

### 插叙：内存操作 API

 **内存类型** 

1. 第一种称为栈内存，它的申请和 释放操作是编译器来隐式管理的，所以有时也称为自动（automatic）内存。

2. 这种对长期内存的需求，所以我们才需要第二种类型的内存，即所谓的堆（heap） 内存，其中所有的申请和释放操作都由程序员显式地完成。

```c
int *x = (int *) malloc(sizeof(int)); 
```

栈和堆的分配都发生在这一行代码中（指针分配在了栈中，但是指向的整数在堆区）：

* 首先编译器看到指针的声明（int * x）时，知道为一个整型指针分配空间

* 随后，当程序调用 malloc()时，它会在堆上请求整数的空间，函数返回这样一个整数的地址（成功时，失败 时则返回 NULL），然后将其存储在栈中以供程序使用。



**malloc()调用 & free(x) ：**

malloc 函数非常简单：传入要申请的堆空间的大小，它成功就返回一个指向新申请空 间的指针，失败就返回 NULL。（malloc()返回一个指向 void 类型的指针，程序员将进一步使用强制类型转换，强制类型转换只是让别人知道在做什么，并不是必须的）

**free()函数**接受一个参数，即一个由 malloc()返回的指针。**分配区域的大小不会被用户传入，必须由内存分配库本身记 录追踪。**



**常见错误**

1. 忘记分配内存（只声明了指针，但是没new空间）
2. 没有分配足够的内存（缓冲区溢出）
3. 忘记初始化分配的内存
4. 忘记释放内存（内存泄露）
5. 在用完之前释放内存（悬挂指针）
6. 反复释放内存（重复释放）
7. 错误地调用 free()

> 为什么在你的进程退出时没有内存泄露，在程序结束时，如果没有释放掉之前申请的内存，但是却没有任何内存会丢失。

**系统中实际存在两级内存管理**

* 第一级是由操作系统执行的内存管理，操作系统在进程运行时将内存交给进程，并在进程退出（或 以其他方式结束）时将其回收。
* 第二级管理在每个进程中，例如在调用 malloc()和 free()时，在堆内管理。 
* 即使你没有调用 free()（并因此泄露了堆中的内存），操作系统也会在程序结束运行时，收回进程的所有 内存（包括用于代码、栈，以及相关堆的内存页）。无论地址空间中堆的状态如何，操作系统都会在进 程终止时收回所有这些页面，从而确保即使没有释放内存，也不会丢失内存。

==对于短时间运行的程序，泄露内存通常不会导致任何操作问题（尽管它可能被认为是不好的 形式）。如果你编写**一个长期运行的服务器**（例如 Web 服务器或数据库管理系统，它永远不会退出）， **泄露内存就是很大的问题，最终会导致应用程序在内存不足时崩溃**。==

* **malloc 库管理虚拟地址空间内的空间**，但是它本身 是建立在一些**系统调用**之上的，**这些系统调用会进入操作系统，来请求更多内存或者将一 些内容释放回系统**。malloc 库管理虚拟地址空间内的空间，但是它本身 是建立在一些系统调用之上的，这些系统调用会进入操作系统，来请求本多内存或者将一 些内容释放回系统。

* **一个这样的系统调用叫作 brk，它被用来改变程序分断（break）的位置：堆结束的位置。 它需要一个参数（新分断的地址），从而根据新分断是大于还是小于当前分断，来增加或减小堆的大小。另一个调用 sbrk 要求传入一个增量，但目的是类似的。** ==请注意，你不应该直接调用 brk 或 sbrk。它们被内存分配库使用。如果你尝试使用它们， 很可能会犯一些错误。建议坚持使用 malloc()和 free()。==

* 还可以通过 mmap()调用从操作系统获取内存
* 其他调用：calloc()，realloc()

### 机制：地址转换

**实现 CPU 虚拟化时，我们遵循的一般准则被称为受限直接访问（Limited Direct  Execution，LDE）**

LDE 背后的想法很简单：==让程序运行的大部分指令直接访问硬件，只在 一些关键点（如进程发起系统调用或发生时钟中断）由操作系统介入来确保“在正确时间， 正确的地点，做正确的事”。为了实现高效的虚拟化，操作系统应该尽量让程序自己运行， 同时通过在关键点的及时介入（interposing），来保持对硬件的控制。**高效和控制**是现代操 作系统的两个主要目标。==

在实现虚拟内存时，我们将追求类似的战略

* **高效**决定了我们要利用**硬件的支持**

* **控制**意味着**操作系统**要确保应用程序只能访问它自己的内存空间，要保护应用程序不会相互影响，也不会影响 操作系统，我们需要硬件的帮助。

* 最后，我们对虚拟内存还有一点要求，即**灵活性**。具体 来说，我们希望程序能以任何方式访问它自己的地址空间，从而让系统更容易编程。

  > **操作系统和硬件需要一块才能实现**

  

**关键问题：如何高效、灵活地虚拟化内存**

==基于硬件的地址转换（hardware-based address  translation），简称为地址转换（address translation）==

**硬件对每次内存访问进行处理（即指令获取、数据读取或写入），将指令中的虚拟（virtual）地址转换为数据实际存储的物理（physical）地址。**因此， 在每次内存引用时，硬件都会进行地址转换，将应用程序的内存引用**重定位**到内存中实际 的位置。

#### 引出问题：。。。

**怎样在内存中重定位这个进程，同时对该进程透明（transparent）？怎么样提供一种虚拟地址 空间从 0 开始的假象，而实际上地址空间位于另外某个物理地址？**

#### 动态（基于硬件）重定位

**具体来说，每个 CPU 需要两个硬件寄存器：基址（base）寄存器和界限（bound）寄存 器，有时称为限制（limit）寄存器。这组基址和界限寄存器，让我们能够将地址空间放在物 理内存的任何位置，同时又能确保进程只能访问自己的地址空间。**

**采用这种方式，==在编写和编译程序时假设地址空间从零开始。但是，当程序真正执行时， 操作系统会决定其在物理内存中的实际加载地址，<font color=red>并将起始地址记录在基址寄存器中</font>==**

进程中使用的内存引用都是虚拟地址（virtual address），硬件接下来将虚拟地址加上**基址寄存器**中的内容，得到物理地址（physical address），再发给内存系统。

**界限寄存器**提供了访问保护。在上面的例子中，界限寄存器被置为 16KB。如果进 程需要访问超过这个界限或者为负数的虚拟地址，CPU 将触发异常，进程最终可能被终止。 界限寄存器的用处在于，它确保了进程产生的所有地址都在进程的地址“界限”中。

* 界限寄存器有两种使用方式，这两种方式在逻辑上是等价的

  * 在一种方式中（像上面那样），它记录**地址空间的大小**，==硬件在将虚拟地址与基址寄存器内容求和前，就检查这个界限。==

  * 另一种方式是界限寄存器中记录**地址空间结束的物理地址**，==硬件在转化虚拟地址到物理地址之后才去检查这个界限。==

**一个基址寄存器将虚拟地址 转换为物理地址，一个界限寄存器确保这个地址在进程地址空间的范围内。它们一起提供了既简单又高 效的虚拟内存机制。**

这种基址寄存器配合界限寄存器的硬件结构是芯片中的**（每个 CPU 一对）**。有时我们将 CPU 的这个负责地址转换的部分统称为**内存管理单元**（Memory Management Unit，MMU）。

**硬件支持：总结 **

* 首先，正如在 CPU 虚拟化的章节中提到 的，我们需要两种 CPU 模式，首先，正如在 CPU 虚拟化的章节中提到 的，我们需要两种 CPU 模式
* 硬件还必须提供基址和界限寄存器（base and bounds register），因此每个 CPU 的内存管 理单元（Memory Management Unit，MMU）都需要这两个额外的寄存器
* 用户程序运行时， 硬件会转换每个地址，即将用户程序产生的虚拟地址加上基址寄存器的内容。硬件也必须 能检查地址是否有用，通过界限寄存器和 CPU 内的一些电路来实现。
* 硬件应该提供一些特殊的指令，用于修改基址寄存器和界限寄存器，允许操作系统在 切换进程时改变它们
* 最后，在用户程序尝试非法访问内存（越界访问）时，CPU必须能够产生异常（exception）。
* CPU 还必须提供一种方法，来通 知它这些处理程序的位置，因此又需要另一些特权指令。





> 动态重定位不仅仅只有硬件的参与，还有操作系统

**硬件支持和操作系统管理结合在一起，实现了一个简单的虚拟内存。**具体来说，在一 些关键的时刻操作系统需要介入，以实现基址和界限方式的虚拟内存

* 第一，在进程创建时，操作系统必须采取行动，为进程的地址空间找到内存空间

* 第二，在进程终止时（正常退出，或因行为不端被强制终止），操作系统也必须做一些 工作，回收它的所有内存，给其他进程或者操作系统使用。

* 第三，在上下文切换时，操作系统也必须执行一些额外的操作。每个 CPU 毕竟只有一 个基址寄存器和一个界限寄存器，但对于每个运行的程序，它们的值都不同，因为每个程 序被加载到内存中不同的物理地址。因此，在切换进程时，操作系统必须保存和恢复基础 和界限寄存器。

  > 当进程停止时（即没有运行），操作系统可以改变其地址空间的物理位置，操作系统首先让进程停止运行，然后将地址空间拷贝到新 位置，最后更新保存的基址寄存器（在进程结构中），指向新位置

* 第四，操作系统必须提供异常处理程序（exception handler），或要一些调用的函数，像 上面提到的那样

总结动态重定位的硬件要求和操作系统的职责

![image-20230428205056092](C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230428205056092.png)

![image-20230428205109338](C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230428205109338.png)

### 分段

利用基址和 界限寄存器，操作系统很容易将不同进程重定位到不同的物理内存区域。但是，对于这些 内存区域，你可能已经注意到一件有趣的事：栈和堆之间，有一大块“空闲”空间。

将整个地址空间放入物理内存，那么栈和堆之间的空间并没有被进程使用，却依然占用了实际的物理内存。因此，**简单的通过基址寄存器和界限寄 存器实现的虚拟内存很浪费。**另外，如果**剩余物理内存无法提供连续区域来放置完整的地址空间，进程便无法运行。**==这种基址加界限的方式看来并不像我们期望的那样灵活。==

为了解决这个问题，分段（segmentation）的概念应运而生。

这个想法很简单，在 MMU 中引入不止 一个基址和界限寄存器对，而是给地址空间内的每个逻辑段（segment）一对。**一个段只是地址空间里的一个连续定长的区域，在典型的地址空间里有 3 个逻辑不同的段：代码、栈 和堆。分段的机制使得操作系统能够将不同的段放到不同的物理内存区域，从而避免了虚拟地址空间中的未使用部分占用物理内存。**

<img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230501091751980.png" alt="image-20230501091751980" style="zoom:50%;" />

<img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230501161656266.png" alt="image-20230501161656266" style="zoom:50%;" />



从图中可以看到，只有已用的内存才在物理内存中分配空间，因此可以容纳巨大的地 址空间，其中包含大量未使用的地址空间（有时又称为稀疏地址空间，sparse address spaces）。

> 补充：段错误 
>
> 段错误指的是在支持分段的机器上发生了非法的内存访问。有趣的是，即使在不支持分段的机器上 这个术语依然保留。但如果你弄不清楚为什么代码老是出错，就没那么有趣了。

**可以看出，分段是物理内存的分段存储，而虚拟内存地址还是图一中所示的那样

**硬件在地址转换时使用段寄存器，**它如何知道段内的偏移量，以及地址引用了哪个段？

* 显式（explicit）方式，就是用虚拟地址的开头几位来标识 不同的段
* 硬件还有其他方法来决定特定地址在哪个段。在隐式（implicit）方式中，硬件通过地 址产生的方式来确定段。例如，如果地址由程序计数器产生（即它是指令获取），那么地址 在代码段。如果基于栈或基址指针，它一定在栈段。其他地址则在堆段。

**栈怎么办？**

> 栈，有一点关键区别，它反向增长。在物理内存中（书中的例子），它始于 28KB， 增长回到 26KB，相应虚拟地址从 16KB 到 14KB。地址转换必须有所不同。

首先，我们需要一点硬件支持。除了基址和界限外，硬件还需要知道段的增长方向（用 一位区分，比如 1 代表自小而大增长，0 反之）

<img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230501161920289.png" alt="image-20230501161920289" style="zoom:50%;" />

#### 支持共享

随着分段机制的不断改进，系统设计人员很快意识到，通过再多一点的硬件支持，就 能实现新的效率提升。具体来说，要节省内存，有时候在地址空间之间共享（share）某些 内存段是有用的。

为了支持共享，需要一些额外的硬件支持，这就是保护位（protection bit）。基本为每个段增加了几个位，标识程序是否能够读写该段，或执行其中的代码。通过将代码段标记为只 读，同样的代码可以被多个进程共享，而不用担心破坏隔离。**虽然每个进程都认为自己独占 这块内存，但操作系统秘密地共享了内存，进程不能修改这些内存，所以假象得以保持。**

有了保护位，前面描述的硬件算法也必须改变。**除了检查虚拟地址是否越界，硬件还 需要检查特定访问是否允许**。如果用户进程试图写入只读段，或从非执行段执行指令，硬 件会触发异常，让操作系统来处理出错进程。

#### 16.5 细粒度与粗粒度的分段 

到目前为止，我们的例子大多针对只有很少的几个段的系统（即代码、栈、堆）。支持许多段需要进一步的硬件支持，并在内存中保存某种**段表**（segment table）。这种段表通常支持创建非常多的段，因此系统使用段的方式，可以比之前讨论的方式更灵活。例如， 像 Burroughs B5000 这样的早期机器可以支持成千上万的段，有了操作系统和硬件的支持，编 译器可以将代码段和数据段划分为许多不同的部分[RK68]。当时的考虑是，**通过更细粒度的 段，操作系统可以更好地了解哪些段在使用哪些没有，从而可以更高效地利用内存。**

#### 16.6 操作系统支持

分段也带来了一些新的问题。我们先介绍必须关注的操作系统新问题

* 操作系统在上下文切换时应该做什么：各个段寄存器中的 内容必须保存和恢复。显然，每个进程都有自己独立的虚拟地址空间，操作系统必须在进 程运行前，确保这些寄存器被正确地赋值。
* 第二个问题更重要，即管理物理内存的空闲空间。新的地址空间被创建时，操作系统 需要在物理内存中为它的段找到空间。之前，我们假设所有的地址空间大小相同，物理内 存可以被认为是一些槽块，进程可以放进去。现在，每个进程都有一些段，每个段的大小 也可能不同
* 外部碎片：一般会遇到的问题是，物理内存很快充满了许多空闲空间的小洞，因而很难分配给新 的段，或扩大已有的段。这种问题被称为外部碎片
  * 该问题的一种解决方案是紧凑（compact）物理内存，重新安排原有的段。这样做，操作系统能让 新的内存分配请求成功。但是，**内存紧凑成本很高，因为拷贝段是内存密集型的，一般会 占用大量的处理器时间。图 16.3（右边）是紧凑后的物理内存。**
  * 一种更简单的做法是利用空闲列表管理算法，试图保留大的内存块用于分配。相关的 算法可能有成百上千种，包括传统的最优匹配（best-fit，从空闲链表中找最接近需要分配空 间的空闲块返回）、最坏匹配（worst-fit）、首次匹配（first-fit）以及像伙伴算法。但遗憾的是，**无论 算法多么精妙，都无法完全消除外部碎片，因此，好的算法只是试图减小它。**

<img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230501231825153.png" alt="image-20230501231825153" style="zoom:50%;" />

外部碎片产生的原因：举个例子，在内存上，分配三个**操作系统分配的用于装载进程的内存区域**A、B和C。假设，三个**内存区域**都是相连的。故而三个**内存区域**不会产生外部碎片。现在假设B对应的进程执行完毕了操作系统随即收回了B，这个时候A和C中间就有一块空闲**区域**了。

[外部碎片和内部碎片的区别](https://blog.csdn.net/haiross/article/details/38704945)

### 空闲空间管理

本章暂且将对虚拟内存的讨论放在一边，来讨论所有内存管理系统的一个基本方面， 无论是 **malloc 库**（管理进程中堆的页），还是**操作系统本身**（管理进程的地址空间）。具体 来说，我们会讨论空闲空间管理（free-space management）的一些问题

空闲空间管理不是只关注上一章说的**操作系统用分段（segmentation） 的方式实现虚拟内存**，还有**用户级的内存分配库（如 malloc()和 free()）**

**本章的大多数讨论，将聚焦于用户级内存分配库中分配程序**

#### 底层机制

如果需要管理的空间被划分为固定大小的单元，就很容易。在这种情况下，**只需要维护这 些大小固定的单元的列表，如果有请求，就返回列表中的第一项。** 如果要管理的空闲空间由大小不同的单元构成，管理就变得困难（而且有趣）

> 关键问题：==如何管理空闲空间要满足**变长**的分配请求==，应该如何管理空闲空间？什么策略可以让碎片最小化？不同方法的时间和 空间开销如何？

> 内部碎片：如果分配程序给出的内存块超出请求的大小，在这种块中超出请求的空间（因此而未使用）就被认为是内部碎片（因为 浪费发生在已分配单元的内部），这是另一种形式的空间浪费，但是在这里不讨论

> 在一些情况下，分配程序 可以要求这块区域增长。例如，一个用户级的内存分配库在空间快用完时，可以向内核申 请增加堆空间（通过 sbrk 这样的系统调用）

**空间分割与合并**

* 分割：**分配程序**会执行所谓的分割（splitting） 动作：**它找到一块可以满足请求的空闲空间，将其分割，第一块返回给用户，第二块留在空闲列表中。**

* 合并：许多分配程序中因此也有一种机制，名为合并。**分配程序会在释放一块内存时合并可用空间。**想法很简单：在归 还一块空闲内存时，仔细查看要归还的内存块的地址以及邻它的空闲空间块。**如果新归还 的空间与一个原有空闲块相邻（或两个，就像这个例子），就将它们合并为一个较大的空闲块。**



**追踪已分配空间的大小** 

free(void *ptr)接口没有块大小的参数。因此它是假定，对于给定的指针， 内存分配库可以很快**确定要释放空间的大小**，从而将它放回空闲列表。

要完成这个任务，大多数分配程序都会在**头块**（header）中保存一点额外的信息，**它在 内存中，通常就在返回的内存块之前。**==该头块中至少包含所分配空间的大小（这个例子中是 20）。它也可能包含一些额外的指 针来加速空间释放，包含一个幻数来提供完整性检查，以及其他信息。==

```c
typedef struct header_t { 
 int size; 
 int magic; 
} header_t;
```

用户调用 free(ptr)时，库会通过简单的指针运算 得到头块的位置： 

```c
void free(void *ptr) 
{  
    header_t *hptr = (void *)ptr - sizeof(header_t);  
}
```

<img src="C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230502203806566.png" alt="image-20230502203806566" style="zoom:67%;" />

获得头块的指针后，库可以很容易地确定幻数是否符合预期的值，作为正常性检查 （assert（hptr->magic == 1234567）），并简单计算要释放的空间大小（**即头块的大小加区域长 度**）。**请注意前一句话中一个小但重要的细节：实际释放的是头块大小加上分配给用户的空 间的大小。**



**嵌入空闲列表&&让堆增长**（见p138，不写了，举的例子，没啥好记得）

要注意的一点：在空闲内存自己内部建立这样一个列表，所以是 在空闲空间本身中建立空闲空间列表（**嵌入**空闲列表）

如果堆中的内存空 间耗尽，应该怎么办？最简单的方式就是返回失败，返回 NULL

==大多数传统的分配程序会从很小的堆开始，当空间耗尽时，再向操作系统申请更大的空 间。通常，这意味着它们进行了某种系统调用（例如，大多数 UNIX 系统中的 sbrk），让堆增 长。操作系统在执行 sbrk 系统调用时，会找到空闲的物理内存页，将它们映射到请求进程的 地址空间中去，并返回新的堆的末尾地址。==

#### 基本策略

理想的分配程序可以同时保证快速和碎片最小化

遗憾的是，由于分配及释放的请求 序列是任意的（毕竟，它们由用户程序决定），任何特定的策略在某组不匹配的输入下都会 变得非常差。所以我们不会描述“最好”的策略，而是介绍一些基本的选择，并讨论它们 的优缺点。

* 最优匹配 
  * 最优匹配（best fit）策略非常简单：首先遍历整个空闲列表，找到和请求大小一样或更大的空闲块，然后返回这组候选者中最小的一块。这就是所谓的最优匹配（也可以称为最小匹配）。只需要遍历一次空闲列表，就足以找到正确的块并返回。
  * 最优匹配背后的想法很简单：选择最接它用户请求大小的块，从而尽量避免空间浪费。然而，这有代价。简单的实现在遍历查找正确的空闲块时，要付出较高的性能代价。
* 最差匹配 
  * 最差匹配（worst fit）方法与最优匹配相反，它尝试找最大的空闲块，分割并满足用户需求后，将剩余的块（很大）加入空闲列表。
  * 最差匹配尝试在空闲列表中保留较大的块，而不是向最优匹配那样可能剩下很多难以利用的小块。但是，最差匹配同样需要遍历整个空闲列表。更糟糕的是，大多数研究表明它的表现非常差，导致过量的碎片，同时还有很高的开销。
* 首次匹配 
  * 首次匹配（first fit）策略就是找到第一个足够大的块，将请求的空间返回给用户。同样，剩余的空闲空间留给后续请求。
  * 首次匹配有速度优势（不需要遍历所有空闲块），但有时会让空闲列表开头的部分有很多小块。因此，分配程序如何管理空闲列表的顺序就变得很重要。一种方式是基于地址排序（address-based ordering）。通过保持空闲块按内存地址有序，合并操作会很容易，从而减少了内存碎片。
* 下次匹配 
  * 不同于首次匹配每次都从列表的开始查找，下次匹配（next fit）算法多维护一个指针，指向上一次查找结束的位置。其想法是将对空闲空间的查找操作扩散到整个列表中去，避免对列表开头频繁的分割。
  * 这种策略的性能与首次匹配很接它，同样避免了遍历查找。

#### 其他方式

.......

### 分页：介绍

**页表**：

为了记录地址空间的每个虚拟页放在物理内存中的位置，操作系统通常为**每个进程保 存一个数据结构**，称为页表（page table）。页表的主要作用是为地址空间的每个虚拟页面保 存地址转换（address translation），从而让我们知道每个页在物理内存中的位置。

系统中每个进程都有 一个页表。页表的确切结构要么由硬件（旧系统）确定，要么由 OS（现代系统）更灵活地管理。

重要的是要记住，**这个页表是一个每进程的数据结构**（我们讨论的大多数页表结构都 是每进程的数据结构，我们将接触的一个例外是倒排页表，inverted page table）。如果在上 面的示例中运行另一个进程，操作系统将不得不为它管理不同的页表，因为它的虚拟页显 然映射到不同的物理页面（除了共享之外）。

**为了转换（translate）该过程生成的虚拟地址，我们必须首先将它分成两个组件：虚拟 页面号（virtual page number，VPN）和页内的偏移量（offset）。**

==用 PFN 替换 VPN 来转换此虚拟地址， 然后将载入发送给物理内存 **请注意，偏移量保持不变**==

物理帧号（PFN）（有时也称为物理页号，physical page number 或 PPN）     虚拟页号（VPN）

==**页表的组织**：页表就是一种数据结构，用于将虚拟地址（或者实际上， 是虚拟页号）映射到物理地址（物理帧号）。==

### 分页：快速地址转换（TLB）

**使用分页作为核心机制来实现虚拟内存，可能会带来较高的性能开销。**因为要使用分 页，就要将内存地址空间切分成大量固定大小的单元（页），并且需要记录这些单元的地址 映射信息。因为这些映射信息一般存储在物理内存中，所以在转换虚拟地址时，分页逻辑 上需要一次额外的内存访问。**每次指令获取、显式加载或保存，都要额外读一次内存以得 到转换信息，这慢得无法接受**

**地址转换旁路缓冲存储器**（translation-lookaside  buffer，TLB[CG68,C95]），它就是频繁发生的虚拟到物理地址转换的硬件缓存（cache）。因 此，更好的名称应该是**地址转换缓存**（address-translation cache）。对每次内存访问，**硬件先检查 TLB，看看其中是否有期望的转换映射，如果有，就完成转换（很快），不用访问页表** （其中有全部的转换映射）。TLB 带来了巨大的性能提升，实际上，因此它使得虚拟内存成 为可能.

**时间局部性&&空间局部性**

* 时间局部性是指，最近访问过的指令或数据项可能 很快会再次访问。想想循环中的循环变量或指令，它们被多次反复访问。
* 空间局部性是指，当程序访问内存地址 x 时，可能很快会访问邻近 x 的内存。想想遍历某种数组，访问一个接一个的元素。

**CISC和RISC**

* CISC 指令集倾向于拥有许多指令，每条指令比较强大。例如，你可能看到一个字符串拷贝，它接 受两个指针和一个长度，将一些字节从源拷贝到目标。CISC 背后的思想是，指令应该是高级原语，这 让汇编语言本身更易于使用，代码更紧凑。 
* RISC 指令集恰恰相反。RISC 背后的关键观点是，指令集实际上是编译器的最终目标，所有编译 器实际上需要少量简单的原语，可以用于生成高性能的代码。因此，RISC 倡导者们主张，尽可能从硬 件中拿掉不必要的东西（尤其是微代码），让剩下的东西简单、统一、快速。



**关键问题**：进程切换时如何管理 TLB 的内容 如果发生进程间上下文切换，上一个进程在 TLB 中的地址映射对于即将运行的进程是无意义的。 硬件或操作系统应该做些什么来解决这个问题呢？

**缓存替换: TLB 替换策略**:在向 TLB 添加新项时，应该替换哪个旧项？目标当然是减小 TLB 未命中率（或提高命中率），从 而改进性能。

**替换最近最少使用（least-recently-used，LRU）** **随机（random）策略**

**TLB 表项**

![image-20230507153602095](C:\Users\痞妖\AppData\Roaming\Typora\typora-user-images\image-20230507153602095.png)

MIPS R4000 支持 32 位的地址空间，页大小为 4KB。所以在典型的虚拟地址中，预期 会看到 20 位的 VPN 和 12 位的偏移量。但是，你可以在 TLB 中看到，==**只有 19 位的 VPN。 事实上，用户地址只占地址空间的一半（剩下的留给内核），所以只需要 19 位的 VPN**==。VPN 转换成最大 24 位的物理帧号（PFN），因此可以支持最多有 64GB 物理内存（224个 4KB 内 存页）的系统

MIPS 的 TLB 通常有 32 项或 64 项，大多数提供给用户进程使用，也有一小部分留给操 作系统使用。操作系统可以设置一个被监听的寄存器，告诉硬件需要为自己预留多少 TLB 槽。这些保留的转换映射，被操作系统用于关键时候它要使用的代码和数据，在这些时候， TLB 未命中可能会导致问题（例如，在 TLB 未命中处理程序中）。

MIPS一些更新 TLB 的指令:

* TLBP，用来查找指定的转换映射是否在 TLB 中；
* TLBR，用来将 TLB 中的内容读取到指定寄存器中；
* TLBWI，用来替换指定的 TLB 项；
* TLBWR，用来随机替 换一个 TLB 项

[**物理高速缓存**](https://blog.csdn.net/dai_xiangjun/article/details/120178369)

### 分页：较小的表

通常系统中的每个 进程都有一个页表！有一百个活动进程（在现代系统中并不罕见），就要为页表分配数百兆 的内存！

混合方法：分页和分段 

多级页表：它只是让线性页表的一部分消 失（释放这些帧用于其他用途），并用页目录来记录页表的哪些页被分配。

PDE：它由多个页目录项（Page  Directory Entries，PDE）组成。PDE（至少）拥有有效位（valid bit）和页帧号（page frame number， PFN），类似于 PTE。

#### 反向页表

我们保留了一个页表，而不是有许多页表，其中的项代表系统的每个物理页，页表项告诉我们哪个进程正在使用此页，以及该进程的哪个虚拟页映射到此物理页。

### 超越物理内存：机制

为了支持更大的地址空间，操作系统需要把 **当前没有在用**的那部分地址空间找个地方存储起来

交换空间  我们要做的第一件事情就是，在硬盘上开辟一部分空间用于物理页的移入和移出。 操作系统中，一般这样的空间称为**交换空间**（swap space），因为我们将内存中的页交换到 其中，并在需要的时候又交换回去。因此，我们会假设操作系统能够**以页大小为单元**读取 或者写入交换空间。为了达到这个目的，**操作系统需要记住给定页的硬盘地址**

具体来说，当硬件在 PTE 中查找时，可能发现页不在物理内存中。硬件（或操作系统，在软件管理 TLB 时）判断是否在内存中的方法，是通过**页表项**中的一条新信息，即**存在位**（present bit）。如果存在位设 置为 1，则表示该页存在于物理内存中，并且所有内容都如上所述进行。如果存在位设置为 零，则页不在内存中，而在硬盘上。访问不在物理内存中的页，这种行为通常被称为页错 误（page fault）。

#### 页错误

硬件管理的 TLB（硬件 在页表中找到需要的转换映射）和软件管理的 TLB（操作系统执行查找过程）。不论在哪种 系统中，如果页不存在，**都由操作系统负责处理页错误**。

当硬盘 I/O 完成时，操作系统会更新页表，将此页标记为存在，更新页表项（PTE）的 PFN 字段以记录新获取页的内存位置，并重试指令。下一次重新访问 TLB 还是未命中，然 而这次因为页在内存中，因此会将页表中的地址更新到 TLB 中（也可以在处理页错误时更 新 TLB 以避免此步骤）。最后的重试操作会在 TLB 中找到转换映射，从已转换的内存物理 地址，获取所需的数据或指令。

**请注意，当 I/O 在运行时，进程将处于阻塞（blocked）状态。因此，当页错误正常处 理时，操作系统可以自由地运行其他可执行的进程。因为 I/O 操作是昂贵的，一个进程进行 I/O（页错误）时会执行另一个进程，这种交叠（overlap）是多道程序系统充分利用硬件的 一种方式**（有点像流水线）

#### 页交换策略

选择哪些页被交换出或被替换（replace）的过程，被称为页交换策略

#### TLB 未命中发生的时候有 3 种重要情景

* 第一种情况，**该页存在（present）且有效（valid）**（第 18～21 行）。在这种情况下，TLB 未 命中处理程序可以简单地从 PTE 中获取 PFN，然后重试指令（这次 TLB 会命中），并因此 继续前面描述的流程。
* 第二种情况（第 22～23 行），**页错误处理程序需要运行**。虽然这是 进程可以访问的合法页（毕竟是有效的），但它并不在物理内存中。
* 第三种情况，**访问的是一个无效页**，可能由于程序中的错误（第 13～14 行）。在这种情况下，PTE 中的其他位都 不重要了。硬件捕获这个非法访问，操作系统陷阱处理程序运行，可能会杀死非法进程。

#### 为了处理页错误，操作系统大致做了什么。

* 首先， 操作系统必须为将要换入的页找到一个物理帧，如果没有这样的物理帧，我们将不得不等 待交换算法运行，并从内存中踢出一些页，释放帧供这里使用。
* 在获得物理帧后，处理程 序发出 I/O 请求从交换空间读取页。
* 最后，当这个慢操作完成时，操作系统更新页表并重试 指令。重试将导致 TLB 未命中，然后再一次重试时，TLB 命中，此时硬件将能够访问所需 的值

#### 交换何时真正发生

操作系统可以更主动地预留一小部分空闲内存

为了保证有少量的空闲内存，大多数操作系统会设置高水位线（High Watermark，HW） 和低水位线（Low Watermark，LW），来帮助决定何时从内存中清除页。原理是这样：==当操 作系统发现有少于 LW 个页可用时，后台负责释放内存的线程会开始运行，直到有 HW 个 可用的物理页。这个后台线程有时称为**交换守护进程**（swap daemon）或**页守护进程**（page  daemon）①，它然后会很开心地进入休眠状态，因为它毕竟为操作系统释放了一些内存。==

交换算法需要先简单检 查是否有空闲页，**而不是直接执行替换**。如果没有空闲页，会通知后台分页线程按需要释 放页。**当线程释放一定数目的页时，它会重新唤醒原来的线程，然后就可以把需要的页交换进内存，继续它的工作。**

### 超越物理内存：策略



















